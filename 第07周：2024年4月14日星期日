首周尝试先使用torch和transforms库写一个简单的LeNet5，并使用MINST数据集训练模型
下一周主要解决的问题（不能使用库）
1. 数据加载和预处理：手动编写代码来加载数据集，并对数据进行适当的预处理。包括读取图像文件、解析标签、将图像数据转换为张量，并进行归一化等操作。
2. 卷积和池化层的实现：LeNet-5模型包含多个卷积层和池化层。需要实现这些层的前向传播和反向传播过程，包括卷积运算、池化运算以及相关的激活函数。
3. 参数初始化：在模型训练之前，需要对模型的参数进行初始化。通常使用随机初始化的方法来初始化权重和偏置。
4. 损失函数的计算：LeNet-5模型通常使用交叉熵损失作为训练的目标函数。需要实现交叉熵损失的计算，并将其作为模型训练的目标。
5. 梯度计算和参数更新：在反向传播过程中，需要手动计算梯度，并使用梯度下降等优化算法来更新模型的参数。
6. 学习率调度：为了提高模型的训练效果，通常会使用学习率调度器来动态调整学习率。需要实现学习率调度器，并在训练过程中适时更新学习率。
-----------------------------------------------------------------------------------------------------------------------------------
第一层，卷积层（C1）
输入原始的图像像素，LeNet-5模型接受的输入层大小为32x32x1。第一个卷积层过滤器的尺寸为5x5，深度为6不使用全0填充，步长为1。因为没有使用全0填充，所以这 一 层的输出的尺寸为32-5+1=28，深度为6。
这一个卷积层总共有5x5x1x6+6=156 个参数，其中6个为偏置项参数。因为下一层的节点矩阵有28x28x6=4704个节点，每个节点和5x5=25个当前层节点相连，所以本层卷积层总共有4704x(25+1)= 122304个连接 。

第二层，池化层（S2）
这一层的输入为第一层的输出，是一个28x28x6的节点矩阵。本层采用的过滤器大小为2x2，长和宽的步长均为2，所以本层的输出矩阵大小为14x14x6。

第三层，卷积层（C3）
本层的输入矩阵大小为14x14x6，使用的过滤器大小为5x5，深度为16。本层不使用全0填充，步长为1。本层的输出矩阵大小为 10x10x16。按照标准的卷积层 ，本层应该有5x5x6x16+16=2416个参数，10x10x16(25+1)=41600个连接 。

第四层，池化层（S4）
本层的输入矩阵大小为10x10x16，采用的过滤器大小为2×2，步长为2。本层的输出矩阵大小为5x5x16。

第五次，卷积层（C5）
本层的输入矩阵大小为5x5x16，使用的过滤器的大小为5×5，深度为120,。本层不使用全0填充，步长为1.本层的输出矩阵大小为1x1x120。总共有5x5x16x120+120=48120个参数。

第六层，全连接层（F6）
本层的输入节点个数为120个，输出节点个数为84个，总共参数为120x84+84=10164个。

第七层，全连接层（Output）
本层的输入节点个数为84个，输出节点个数为10 个，分别代表数字0到9，总共参数为84x10+10=850个。LeNet-5模型论文中最后一层输出层的结构和全连接层有区别，但我们这用全连接层近似的表示 。
-----------------------------------------------------------------------------------------------------------------------------------
定义出LeNet5最基本的搭建形式：

class MyLeNet5(nn.Module):
    def __init__(self):
        super(MyLeNet5, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = torch.relu(x)
        x = self.avgpool1(x)
        x = self.conv2(x)
        x = torch.relu(x)
        x = self.avgpool2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        x = torch.relu(x)
        x = self.fc3(x)
        return x

关键训练函数：
def train(dataloader, model, loss_fn, optimizer):
    loss, current, n = 0.0, 0.0, 0
    for batch, (X, y) in enumerate(dataloader):
        # 前向转播
        X, y = X.to(device), y.to(device)
        output = model(X)
        cur_loss = loss_fn(output, y)
        _, pred = torch.max(output, axis=1)

        cur_acc = torch.sum(y == pred)/output.shape[0]

        optimizer.zero_grad()
        cur_loss.backward()
        optimizer.step()

        loss += cur_loss.item()
        current += cur_acc.item()
        n = n + 1
    print("train_loss" + str(loss/n))
    print("train_acc" + str(current/n))

def val(dataloader, model, loss_fn):
    model.eval()
    loss, current, n = 0.0, 0.0, 0
    with torch.no_grad():
        for batch, (X, y) in enumerate(dataloader):
            # 前向转播
            X, y = X.to(device), y.to(device)
            output = model(X)
            cur_loss = loss_fn(output, y)
            _, pred = torch.max(output, axis=1)
            cur_acc = torch.sum(y == pred) / output.shape[0]
            loss += cur_loss.item()
            current += cur_acc.item()
            n = n + 1
        print("val_loss" + str(loss / n))
        print("val_acc" + str(current / n))

        return current/n

测试形式：
for i in range(5):
    X, y = test_dataset[i][0], test_dataset[i][1]
    show(X).show()

    X = Variable(torch.unsqueeze(X, dim=0).float(), requires_grad=False).to(device)
    with torch.no_grad():
        pred = model(X)
        predicted, actual = classes[torch.argmax(pred[0])], classes[y]
        print(f'predicted: "{predicted}", actual:"{actual}"')
