克服的难关1（前向后向传播）：

前向传播（Forward Propagation）：
前向传播是指从神经网络的输入开始，逐层计算并传递数据，直到得到输出结果的过程。它涉及以下数学公式：
线性变换：对于每一层的神经元，首先进行线性变换，计算加权输入。假设输入为x，权重为W，偏置为b，则线性变换可以表示为：
Z = W * x + b
这里，* 表示矩阵乘法。
激活函数：线性变换后，将其输入到激活函数中。激活函数引入非线性变换，增加网络的表达能力。常见的激活函数包括ReLU、Sigmoid和Tanh等。以ReLU激活函数为例，其计算公式为：
A = max(0, Z)
其中，max函数选择输入和0之间的较大值。
重复以上步骤：对于每一层，重复进行线性变换和激活函数的计算，直到达到输出层。最终得到输出结果。

后向传播（Backward Propagation）：
后向传播是通过计算损失函数关于网络参数的梯度，从输出层向输入层进行反向传播梯度的过程。它涉及以下数学公式：
损失函数（Loss Function）：选择适当的损失函数来度量模型输出与真实标签之间的差异。例如，对于回归问题可以使用均方误差（Mean Squared Error），对于分类问题可以使用交叉熵损失（Cross-Entropy Loss）。
反向传播：通过链式法则，计算损失函数对于每个参数的梯度。假设L表示损失函数，W表示权重参数，b表示偏置参数，A表示激活函数的输出，Z表示线性变换的结果，那么反向传播可以表示为：
dL/dZ = dL/dA * dA/dZ
dL/dW = dZ/dW * dL/dZ
dL/db = dZ/db * dL/dZ
这里，dL/dZ 表示损失函数对于 Z 的梯度，dL/dW 表示损失函数对于 W 的梯度，dL/db 表示损失函数对于 b 的梯度，dA/dZ 表示激活函数对于 Z 的导数。
参数更新：根据计算得到的梯度，使用优化算法（如随机梯度下降）更新网络参数，使损失函数逐渐减小。

克服的难关2（前向后向传播）：
